{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start...\n",
      "Training model...\n",
      "Doc2Vec(dm/s,d300,hs,w8,mc5)\n",
      "Hurray\n",
      "(9001, 100)\n",
      "clustering...\n",
      "Time taken for K Means clustering:  5.36599993706 seconds.\n",
      "\n",
      "Cluster 0\n",
      "\n",
      "Cluster 1\n",
      "\n",
      "Cluster 2\n",
      "\n",
      "Cluster 3\n",
      "\n",
      "Cluster 4\n",
      "\n",
      "Cluster 5\n",
      "\n",
      "Cluster 6\n",
      "\n",
      "Cluster 7\n",
      "\n",
      "Cluster 8\n",
      "\n",
      "Cluster 9\n",
      "\n",
      "Cluster 10\n",
      "\n",
      "Cluster 11\n",
      "\n",
      "Cluster 12\n",
      "\n",
      "Cluster 13\n",
      "\n",
      "Cluster 14\n",
      "\n",
      "Cluster 15\n",
      "\n",
      "Cluster 16\n",
      "\n",
      "Cluster 17\n",
      "\n",
      "Cluster 18\n",
      "\n",
      "Cluster 19\n",
      "\n",
      "Cluster 20\n",
      "\n",
      "Cluster 21\n",
      "\n",
      "Cluster 22\n",
      "\n",
      "Cluster 23\n",
      "\n",
      "Cluster 24\n",
      "\n",
      "Cluster 25\n",
      "\n",
      "Cluster 26\n",
      "\n",
      "Cluster 27\n",
      "\n",
      "Cluster 28\n",
      "\n",
      "Cluster 29\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import codecs\n",
    "import logging\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "import time\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "print 'start...'\n",
    "    \n",
    "def create_bag_of_centroids( wordlist, word_centroid_map ):\n",
    "    num_centroids = max( word_centroid_map.values() ) + 1\n",
    "    bag_of_centroids = np.zeros( num_centroids, dtype=\"float32\" )\n",
    "    for word in wordlist:\n",
    "        if word in word_centroid_map:\n",
    "            index = word_centroid_map[word]\n",
    "            bag_of_centroids[index] += 1\n",
    "    return bag_of_centroids\n",
    " \n",
    " \n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "     \n",
    "num_features = 100  # Word vector dimensionality                      \n",
    "min_word_count = 10  # Minimum word count                        \n",
    "num_workers = 4     # Number of threads to run in parallel\n",
    "context = 10         # Context window size                                                                                    \n",
    "downsampling = 1e-3 # Downsample setting for frequent words\n",
    " \n",
    "sentences = []\n",
    "f = codecs.open(r'C:\\Users\\malts_000\\Desktop\\test\\learn_test.txt', 'r', 'utf8').read().split('\\n')\n",
    "sent_lab = 0\n",
    "for s in f:\n",
    "    sentences.append(LabeledSentence(s.split(), 'SENT_%s'%sent_lab))\n",
    "    sent_lab += 1\n",
    "    \n",
    "train_ = sentences[:9000]\n",
    "\n",
    "\n",
    "print \"Training model...\"\n",
    "#\n",
    "model = Doc2Vec(train_,\n",
    "                alpha=0.025,\n",
    "                min_alpha=0.025)\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train(train_)\n",
    "model.train(train_)\n",
    "\n",
    "model_name = \"test_coursepaper\"\n",
    "model.save(model_name)\n",
    "print model\n",
    "\n",
    "print \"Hurray\"\n",
    "# model = gensim.models.Word2Vec.load('/Users/grach/Downloads/experiment/test_enjoyme')\n",
    " \n",
    "start = time.time() \n",
    "word_vectors = model.syn0\n",
    " \n",
    "#print model.shape()\n",
    "num_clusters = 200 #это можно регулировать, для получения наилучших результатов\n",
    " \n",
    "queriesNP = np.zeros((1,100), dtype='float32') \n",
    " \n",
    "for i in train_:\n",
    "    tnp = np.zeros((1,100), dtype='float32')\n",
    "    # print i\n",
    "    for j in i:\n",
    "        try:\n",
    "            tnp += model[j]\n",
    "            # print 'hurray'\n",
    "        except:\n",
    "            continue\n",
    "            # print j +  ' not in vocab'\n",
    "    queriesNP = np.vstack((queriesNP, tnp))\n",
    "\n",
    " \n",
    " \n",
    "np.save('outfile', queriesNP)\n",
    "# np.load('outfile')\n",
    " \n",
    "print queriesNP.shape\n",
    " \n",
    "start = time.time() \n",
    " \n",
    "print 'clustering...'\n",
    "         \n",
    "# Initalize a k-means object and use it to extract centroids\n",
    "kmeans_clustering = KMeans( n_clusters = num_clusters )\n",
    "idx = kmeans_clustering.fit_predict( queriesNP[1:] )\n",
    " \n",
    "# Get the end time and print how long the process took\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print \"Time taken for K Means clustering: \", elapsed, \"seconds.\"\n",
    " \n",
    "# word_centroid_map = dict(zip( model.index2word, idx ))\n",
    "text_out = []\n",
    "for i in sentences[9000:]:\n",
    "    tmp_str = ''\n",
    "    for j in i:\n",
    "        tmp_str = j\n",
    "    text_out.append(tmp_str)\n",
    "         \n",
    "     \n",
    "word_centroid_map = dict(zip( text_out, idx ))\n",
    " \n",
    "for cluster in xrange(0,30):\n",
    "    print \"\\nCluster %d\" % cluster\n",
    "    \n",
    "    words = []\n",
    "    for i in xrange(0,len(word_centroid_map.values())):\n",
    "        if( word_centroid_map.values()[i] == cluster ):\n",
    "            words.append(word_centroid_map.keys()[i])\n",
    "    f = codecs.open(r'C:\\Users\\malts_000\\Desktop\\test\\clusters\\cluster_%s.txt'%cluster, 'w', 'utf8')\n",
    "    for i in words: f.write(i + '\\n')\n",
    "    f.close()\n",
    "# modeld2v = doc2vec.load_word2vec_format(fname, fvocab=None, binary=False, encoding='utf8', unicode_errors='strict')\n",
    " \n",
    "# for i in output[9]:\n",
    "#     print i\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LabeledSentence([u'\\u0417\\u043d\\u0430\\u043b\\u0438', u'\\u043b\\u0438', u'\\u0432\\u044b,', u'\\u0447\\u0442\\u043e', u'\\u0432\\u043a\\u043e\\u043d\\u0442\\u0430\\u043a\\u0442\\u0435', u'\\u0434\\u043e', u'\\u0441\\u0438\\u0445', u'\\u043f\\u043e\\u0440', u'\\u043c\\u043e\\u0436\\u043d\\u043e', u'\\u0432\\u0435\\u0448\\u0430\\u0442\\u044c', u'\\u043d\\u0430', u'\\u0441\\u0442\\u0435\\u043d\\u0443', u'\\u0433\\u0440\\u0430\\u0444\\u0444\\u0438\\u0442\\u0438?'], SENT_0)\n"
     ]
    }
   ],
   "source": [
    "print(sentences[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
